{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Environment Setup & Imports",
   "id": "182b8b261d5385a0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-29T14:26:55.314328Z",
     "start_time": "2025-01-29T14:26:55.301319Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T14:26:55.345874Z",
     "start_time": "2025-01-29T14:26:55.331359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download NLTK resources\n",
    "nltk.download(['stopwords', 'wordnet', 'punkt', 'averaged_perceptron_tagger'])"
   ],
   "id": "10ca1f19163a08d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KomPhone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KomPhone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KomPhone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\KomPhone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T14:26:55.392874Z",
     "start_time": "2025-01-29T14:26:55.385884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ],
   "id": "ee3a289bbfc249a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Data Loading & Preprocessing",
   "id": "ea88e1aadaf86db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T14:28:44.112951Z",
     "start_time": "2025-01-29T14:26:55.425739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('datasets/all_it_jobs.csv')\n",
    "columns_to_keep = ['review_text', 'sentiment']\n",
    "df = df[columns_to_keep].dropna(subset=['review_text'])\n",
    "# Balance dataset with replacement\n",
    "df_sampled = (df.groupby(\"sentiment\")\n",
    "              .sample(n=20000, random_state=42, replace=True)\n",
    "              .reset_index(drop=True))\n",
    "df_sampled.head(5)\n",
    "# Text preprocessing setup\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    return {\n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|[^a-zA-Z\\s]', '', text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(w, get_wordnet_pos(tag))\n",
    "              for w, tag in pos_tag(nltk.word_tokenize(text))\n",
    "              if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "df_sampled['cleaned_review'] = df_sampled['review_text'].apply(preprocess_text)\n",
    "df_sampled = df_sampled[df_sampled['cleaned_review'].str.strip().astype(bool)]\n"
   ],
   "id": "9ab093f3c035c30d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Data Preparation & Splitting",
   "id": "dc634a7d8befb561"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T14:28:46.440430Z",
     "start_time": "2025-01-29T14:28:44.145951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the data\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_sampled, test_size=0.3, stratify=df_sampled['sentiment'], random_state=42\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df['sentiment'], random_state=42\n",
    ")\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df['cleaned_review'])\n",
    "\n",
    "\n",
    "def prepare_sequences(df):\n",
    "    sequences = tokenizer.texts_to_sequences(df['cleaned_review'])\n",
    "    lengths = [min(len(seq), 250) for seq in sequences]\n",
    "    padded = pad_sequences(sequences, maxlen=250, padding='post', truncating='post')\n",
    "    return padded, lengths\n",
    "\n",
    "\n",
    "X_train, train_lengths = prepare_sequences(train_df)\n",
    "X_val, val_lengths = prepare_sequences(val_df)\n",
    "X_test, test_lengths = prepare_sequences(test_df)\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df['sentiment'])\n",
    "y_val = le.transform(val_df['sentiment'])\n",
    "y_test = le.transform(test_df['sentiment'])"
   ],
   "id": "1b13d74463b81a89",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Embedding Layer Preparation",
   "id": "71168043f73620ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T14:28:54.448078Z",
     "start_time": "2025-01-29T14:28:46.473883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_glove_embeddings(path, tokenizer, embed_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "\n",
    "    vocab_size = tokenizer.num_words + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i >= vocab_size: continue\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "\n",
    "embedding_matrix = load_glove_embeddings('glove.6B.100d.txt', tokenizer, 100)\n"
   ],
   "id": "be2bada94fb76a63",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. PyTorch Dataset & DataLoader",
   "id": "f6f2590aacf2edd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T14:28:54.572238Z",
     "start_time": "2025-01-29T14:28:54.527089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataloader(X, lengths, y, batch_size=128, shuffle=False):\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(X, dtype=torch.long),\n",
    "        torch.tensor(lengths, dtype=torch.long),\n",
    "        torch.tensor(y, dtype=torch.long)\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4)\n",
    "\n",
    "\n",
    "train_loader = create_dataloader(X_train, train_lengths, y_train, shuffle=True)\n",
    "val_loader = create_dataloader(X_val, val_lengths, y_val)\n",
    "test_loader = create_dataloader(X_test, test_lengths, y_test)"
   ],
   "id": "7e62054e28be3a4c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. LSTM Model",
   "id": "4a9967548ec6d0ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T14:28:54.620251Z",
     "start_time": "2025-01-29T14:28:54.606237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim,\n",
    "                 num_layers, dropout, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False) \\\n",
    "            if pretrained_embeddings is not None \\\n",
    "            else nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers,\n",
    "                            dropout=dropout, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        # Forward pass through LSTM\n",
    "        _, (hidden, _) = self.lstm(packed)\n",
    "        \n",
    "        # Split the hidden state into forward and backward directions\n",
    "        num_layers = self.lstm.num_layers\n",
    "        # First num_layers entries are forward, next num_layers are backward\n",
    "        hidden_forward = hidden[:num_layers, :, :]\n",
    "        hidden_backward = hidden[num_layers:, :, :]\n",
    "        \n",
    "        # Extract the last hidden states from both directions\n",
    "        last_hidden_forward = hidden_forward[-1, :, :]  # (batch_size, hidden_dim)\n",
    "        last_hidden_backward = hidden_backward[-1, :, :]  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Concatenate the final forward and backward hidden states\n",
    "        combined = torch.cat((last_hidden_forward, last_hidden_backward), dim=1)\n",
    "        \n",
    "        return self.fc(self.dropout(combined))"
   ],
   "id": "d81bcf831c8b0a97",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. Training & Evaluation Functions",
   "id": "107e9bdbd6b40833"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T14:28:54.667758Z",
     "start_time": "2025-01-29T14:28:54.653761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for X, lengths, y in loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(X, lengths)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == y).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, lengths, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X, lengths)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)"
   ],
   "id": "76bfc3ec45eb4d70",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 8. Hyperparameter Optimization with Optuna",
   "id": "d78d914fe4f81d0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:23:54.843025Z",
     "start_time": "2025-01-29T14:34:40.775681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    params = {\n",
    "        'hidden_dim': trial.suggest_categorical('hidden_dim', [128, 256]),  # Adjusted\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 2),  # Reduced max layers\n",
    "        'dropout': trial.suggest_float('dropout', 0.2, 0.6)  # Higher dropout\n",
    "                  if (num_layers := trial.suggest_int('num_layers', 1, 2)) > 1\n",
    "                  else 0.0,\n",
    "        'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),  # Tighter LR range\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),\n",
    "    }\n",
    "\n",
    "    model = BiLSTM(\n",
    "        vocab_size=10001,\n",
    "        embed_dim=100,\n",
    "        output_dim=len(le.classes_),\n",
    "        pretrained_embeddings=embedding_matrix,\n",
    "        hidden_dim=params['hidden_dim'],\n",
    "        num_layers=params['num_layers'],\n",
    "        dropout=params['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(  # Better optimizer\n",
    "        model.parameters(),\n",
    "        lr=params['lr'],\n",
    "        weight_decay=params['weight_decay']\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_acc = 0\n",
    "    patience = 2\n",
    "    no_improvement = 0\n",
    "\n",
    "    for epoch in range(8):  # Slightly increased epochs\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Report intermediate results for pruning\n",
    "        trial.report(val_acc, epoch)\n",
    "\n",
    "        # Early stopping and pruning\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        # Log metrics\n",
    "        writer.add_scalars('Loss', {'train': train_loss, 'val': val_loss}, epoch)\n",
    "        writer.add_scalars('Accuracy', {'train': train_acc, 'val': val_acc}, epoch)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement >= patience:\n",
    "                break  # Early exit from unpromising trials\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)"
   ],
   "id": "678cff829cc87e12",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-29 14:34:40,783] A new study created in memory with name: no-name-5e017af8-a1a2-4e61-a824-6bc60c249e3b\n",
      "[I 2025-01-29 14:39:09,986] Trial 0 finished with value: 0.6188888888888889 and parameters: {'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.43990518765075687, 'lr': 2.8961622775096578e-05, 'weight_decay': 5.587093958780898e-05}. Best is trial 0 with value: 0.6188888888888889.\n",
      "[I 2025-01-29 14:41:59,376] Trial 1 finished with value: 0.6108888888888889 and parameters: {'hidden_dim': 128, 'num_layers': 1, 'lr': 2.9763195717810546e-05, 'weight_decay': 0.0001793600528400798}. Best is trial 0 with value: 0.6188888888888889.\n",
      "[I 2025-01-29 14:46:03,699] Trial 2 finished with value: 0.6284444444444445 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.27804327781772736, 'lr': 6.248812529954186e-05, 'weight_decay': 3.0910510189933214e-06}. Best is trial 2 with value: 0.6284444444444445.\n",
      "[I 2025-01-29 14:50:21,708] Trial 3 finished with value: 0.6606666666666666 and parameters: {'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.4390534643149119, 'lr': 0.0005681788342175143, 'weight_decay': 2.7906653542071213e-06}. Best is trial 3 with value: 0.6606666666666666.\n",
      "[I 2025-01-29 14:53:04,218] Trial 4 finished with value: 0.628 and parameters: {'hidden_dim': 256, 'num_layers': 1, 'lr': 5.118941710329547e-05, 'weight_decay': 4.86813998509662e-05}. Best is trial 3 with value: 0.6606666666666666.\n",
      "[I 2025-01-29 14:53:24,620] Trial 5 pruned. \n",
      "[I 2025-01-29 14:53:45,400] Trial 6 pruned. \n",
      "[I 2025-01-29 14:55:30,873] Trial 7 finished with value: 0.66 and parameters: {'hidden_dim': 128, 'num_layers': 1, 'lr': 0.0009865540141528217, 'weight_decay': 8.452114202008279e-05}. Best is trial 3 with value: 0.6606666666666666.\n",
      "[I 2025-01-29 14:59:55,522] Trial 8 finished with value: 0.6425555555555555 and parameters: {'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.41680967937072955, 'lr': 9.86255290138149e-05, 'weight_decay': 0.00034959320992498347}. Best is trial 3 with value: 0.6606666666666666.\n",
      "[I 2025-01-29 15:00:30,498] Trial 9 pruned. \n",
      "[I 2025-01-29 15:05:05,030] Trial 10 finished with value: 0.6543333333333333 and parameters: {'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.5911831449974808, 'lr': 0.0004762310945327237, 'weight_decay': 1.163637036009626e-06}. Best is trial 3 with value: 0.6606666666666666.\n",
      "[I 2025-01-29 15:07:18,846] Trial 11 finished with value: 0.6584444444444445 and parameters: {'hidden_dim': 128, 'num_layers': 1, 'lr': 0.0009892701532775597, 'weight_decay': 6.984060933667645e-06}. Best is trial 3 with value: 0.6606666666666666.\n",
      "[I 2025-01-29 15:11:30,194] Trial 12 finished with value: 0.6595555555555556 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.5390165537027845, 'lr': 0.00030187502611847143, 'weight_decay': 7.983767337785504e-06}. Best is trial 3 with value: 0.6606666666666666.\n",
      "[I 2025-01-29 15:13:31,429] Trial 13 finished with value: 0.6592222222222223 and parameters: {'hidden_dim': 128, 'num_layers': 1, 'lr': 0.0009944421010376345, 'weight_decay': 1.604988393144902e-05}. Best is trial 3 with value: 0.6606666666666666.\n",
      "[I 2025-01-29 15:14:55,951] Trial 14 pruned. \n",
      "[I 2025-01-29 15:19:28,626] Trial 15 finished with value: 0.6623333333333333 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.3213271299535587, 'lr': 0.0004345260254699722, 'weight_decay': 1.22096147018767e-06}. Best is trial 15 with value: 0.6623333333333333.\n",
      "[I 2025-01-29 15:20:01,049] Trial 16 pruned. \n",
      "[I 2025-01-29 15:22:49,731] Trial 17 finished with value: 0.6537777777777778 and parameters: {'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.34207570926683345, 'lr': 0.0005022711647880906, 'weight_decay': 2.8229629233848776e-06}. Best is trial 15 with value: 0.6623333333333333.\n",
      "[I 2025-01-29 15:23:22,081] Trial 18 pruned. \n",
      "[I 2025-01-29 15:23:54,832] Trial 19 pruned. \n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 9. Final Model Training",
   "id": "f0bbaf35a55e614b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:29:42.842327Z",
     "start_time": "2025-01-29T15:26:42.919807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_params = study.best_params\n",
    "final_model = BiLSTM(\n",
    "    vocab_size=10001,\n",
    "    embed_dim=100,\n",
    "    output_dim=len(le.classes_),\n",
    "    pretrained_embeddings=embedding_matrix,\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "# Train final model\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(6):\n",
    "    train_loss, train_acc = train_epoch(final_model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(final_model, val_loader, criterion, device)\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}: Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')"
   ],
   "id": "b110fb1d16da4487",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.8693 Acc: 0.5802 | Val Loss: 0.7903 Acc: 0.6270\n",
      "Epoch 2: Train Loss: 0.7825 Acc: 0.6323 | Val Loss: 0.7690 Acc: 0.6400\n",
      "Epoch 3: Train Loss: 0.7497 Acc: 0.6537 | Val Loss: 0.7513 Acc: 0.6507\n",
      "Epoch 4: Train Loss: 0.7234 Acc: 0.6679 | Val Loss: 0.7679 Acc: 0.6418\n",
      "Epoch 5: Train Loss: 0.6994 Acc: 0.6824 | Val Loss: 0.7617 Acc: 0.6451\n",
      "Epoch 6: Train Loss: 0.6806 Acc: 0.6959 | Val Loss: 0.7572 Acc: 0.6589\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 10. Final Evaluation",
   "id": "154c659ead839925"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:30:14.041991Z",
     "start_time": "2025-01-29T15:30:08.864707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss, test_acc = evaluate(final_model, test_loader, criterion, device)\n",
    "print(f'\\nFinal Test Performance: Loss: {test_loss:.4f} | Accuracy: {test_acc:.4f}')"
   ],
   "id": "b7d4b685805f326f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Performance: Loss: 0.7615 | Accuracy: 0.6493\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 11. Save the best parameters",
   "id": "5edba84ee50cf2a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:30:19.117076Z",
     "start_time": "2025-01-29T15:30:19.105068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open('params/best_bi_lstm_params.json', 'w') as f:\n",
    "    json.dump(best_params, f)"
   ],
   "id": "8ca50ccc7f60a3cd",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "907db5da27420eb0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
