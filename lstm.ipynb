{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Environment Setup & Imports",
   "id": "ad370e868b427e7d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-28T20:26:52.561755Z",
     "start_time": "2025-01-28T20:26:44.703256Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:28:07.350864Z",
     "start_time": "2025-01-28T20:28:07.109596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download NLTK resources\n",
    "nltk.download(['stopwords', 'wordnet', 'punkt', 'averaged_perceptron_tagger'])"
   ],
   "id": "99aaee20ae6940d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KomPhone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KomPhone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KomPhone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\KomPhone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:28:11.851823Z",
     "start_time": "2025-01-28T20:28:11.751025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ],
   "id": "8040beedd3f12361",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Data Loading & Preprocessing",
   "id": "1694acd81aea8583"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:28:35.206655Z",
     "start_time": "2025-01-28T20:28:32.435026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('datasets/all_it_jobs.csv')\n",
    "columns_to_keep = ['review_text', 'sentiment']\n",
    "df = df[columns_to_keep].dropna(subset=['review_text'])"
   ],
   "id": "4e81d631c26603f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:28:39.209756Z",
     "start_time": "2025-01-28T20:28:39.066862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Balance dataset with replacement\n",
    "df_sampled = (df.groupby(\"sentiment\")\n",
    "              .sample(n=20000, random_state=42, replace=True)\n",
    "              .reset_index(drop=True))"
   ],
   "id": "42f1a1d025377b5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:29:03.465370Z",
     "start_time": "2025-01-28T20:29:03.438365Z"
    }
   },
   "cell_type": "code",
   "source": "df_sampled.head(5)",
   "id": "711b765332698517",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                         review_text sentiment\n",
       "0  depends on teams avoid proddev bonuses are goo...  Negative\n",
       "1  do not expect more than 15 of hike while joini...  Negative\n",
       "2  disrespectful and ancient generally nice peopl...  Negative\n",
       "3  if you don’t want to mail in the next 30 years...  Negative\n",
       "4  work somewhere else that rewards innovation an...  Negative"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>depends on teams avoid proddev bonuses are goo...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do not expect more than 15 of hike while joini...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disrespectful and ancient generally nice peopl...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you don’t want to mail in the next 30 years...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>work somewhere else that rewards innovation an...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:29:31.895731Z",
     "start_time": "2025-01-28T20:29:31.880742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Text preprocessing setup\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    return {\n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|[^a-zA-Z\\s]', '', text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(w, get_wordnet_pos(tag)) \n",
    "              for w, tag in pos_tag(nltk.word_tokenize(text)) \n",
    "              if w not in stop_words]\n",
    "    return ' '.join(tokens)"
   ],
   "id": "1b87e55204a35dda",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:31:24.039212Z",
     "start_time": "2025-01-28T20:29:40.613408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply preprocessing\n",
    "df_sampled['cleaned_review'] = df_sampled['review_text'].apply(preprocess_text)\n",
    "df_sampled = df_sampled[df_sampled['cleaned_review'].str.strip().astype(bool)]"
   ],
   "id": "f1e7744283a4e4d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Data Preparation & Splitting",
   "id": "7fdcbf028d8ffa93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:31:28.189090Z",
     "start_time": "2025-01-28T20:31:28.101322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the data\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_sampled, test_size=0.3, stratify=df_sampled['sentiment'], random_state=42\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df['sentiment'], random_state=42\n",
    ")"
   ],
   "id": "625e857eabedfd42",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:31:32.591341Z",
     "start_time": "2025-01-28T20:31:30.428923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df['cleaned_review'])\n",
    "\n",
    "def prepare_sequences(df):\n",
    "    sequences = tokenizer.texts_to_sequences(df['cleaned_review'])\n",
    "    lengths = [min(len(seq), 250) for seq in sequences]\n",
    "    padded = pad_sequences(sequences, maxlen=250, padding='post', truncating='post')\n",
    "    return padded, lengths\n",
    "\n",
    "X_train, train_lengths = prepare_sequences(train_df)\n",
    "X_val, val_lengths = prepare_sequences(val_df)\n",
    "X_test, test_lengths = prepare_sequences(test_df)"
   ],
   "id": "d0ad896141133f1b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:31:56.732187Z",
     "start_time": "2025-01-28T20:31:56.714808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df['sentiment'])\n",
    "y_val = le.transform(val_df['sentiment'])\n",
    "y_test = le.transform(test_df['sentiment'])"
   ],
   "id": "b5a4071bc0bd2200",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Embedding Layer Preparation",
   "id": "a4c44a427e136eda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:32:28.570461Z",
     "start_time": "2025-01-28T20:32:20.107710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_glove_embeddings(path, tokenizer, embed_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    \n",
    "    vocab_size = tokenizer.num_words + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "    \n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i >= vocab_size: continue\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "            \n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "embedding_matrix = load_glove_embeddings('glove.6B.100d.txt', tokenizer, 100)"
   ],
   "id": "6ad0f4c7f6627028",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. PyTorch Dataset & DataLoader",
   "id": "6e4bb605306bdf74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:32:41.900193Z",
     "start_time": "2025-01-28T20:32:41.858195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataloader(X, lengths, y, batch_size=128, shuffle=False):\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(X, dtype=torch.long),\n",
    "        torch.tensor(lengths, dtype=torch.long),\n",
    "        torch.tensor(y, dtype=torch.long)\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4)\n",
    "\n",
    "train_loader = create_dataloader(X_train, train_lengths, y_train, shuffle=True)\n",
    "val_loader = create_dataloader(X_val, val_lengths, y_val)\n",
    "test_loader = create_dataloader(X_test, test_lengths, y_test)"
   ],
   "id": "91f1bd2515a6210c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. LSTM Model",
   "id": "e505ab17425a2531"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:33:01.495356Z",
     "start_time": "2025-01-28T20:33:01.479088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, \n",
    "                 num_layers, dropout, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False) \\\n",
    "            if pretrained_embeddings is not None \\\n",
    "            else nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
    "                           dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, (hidden, _) = self.lstm(packed)\n",
    "        return self.fc(self.dropout(hidden[-1]))"
   ],
   "id": "48ac31ee2e4046fd",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. Training & Evaluation Functions",
   "id": "9399b11cfb37687"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:33:36.254309Z",
     "start_time": "2025-01-28T20:33:36.244579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for X, lengths, y in loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(X, lengths)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == y).sum().item()\n",
    "        \n",
    "    return total_loss/len(loader), correct/len(loader.dataset)"
   ],
   "id": "e3ce276cdbd20122",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:33:36.772191Z",
     "start_time": "2025-01-28T20:33:36.759855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, lengths, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X, lengths)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            \n",
    "    return total_loss/len(loader), correct/len(loader.dataset)"
   ],
   "id": "d4c6598675d36112",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 8. Hyperparameter Optimization with Optuna",
   "id": "c61446ef5fd663d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T20:34:00.404386Z",
     "start_time": "2025-01-28T20:34:00.384389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    params = {\n",
    "        'hidden_dim': trial.suggest_categorical('hidden_dim', [128, 256, 512]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 3),\n",
    "        # Conditional dropout - only suggest when num_layers > 1\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.5) \n",
    "                   if trial.suggest_int('num_layers', 1, 3) > 1 \n",
    "                   else 0.0,\n",
    "        'lr': trial.suggest_float('lr', 1e-5, 1e-2, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256])\n",
    "    }\n",
    "    \n",
    "    model = SentimentLSTM(\n",
    "        vocab_size=10001,\n",
    "        embed_dim=100,\n",
    "        output_dim=len(le.classes_),\n",
    "        pretrained_embeddings=embedding_matrix,\n",
    "        hidden_dim=params['hidden_dim'],\n",
    "        num_layers=params['num_layers'],\n",
    "        dropout=params['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(  # Better optimizer\n",
    "        model.parameters(), \n",
    "        lr=params['lr'],\n",
    "        weight_decay=trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)  # Added\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_acc = 0\n",
    "    patience = 2\n",
    "    no_improvement = 0\n",
    "\n",
    "    for epoch in range(15):  # Slightly increased epochs\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Report intermediate results for pruning\n",
    "        trial.report(val_acc, epoch)\n",
    "        \n",
    "        # Early stopping and pruning\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "        \n",
    "        # Log metrics\n",
    "        writer.add_scalars('Loss', {'train': train_loss, 'val': val_loss}, epoch)\n",
    "        writer.add_scalars('Accuracy', {'train': train_acc, 'val': val_acc}, epoch)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement >= patience:\n",
    "                break  # Early exit from unpromising trials\n",
    "\n",
    "    return best_val_acc"
   ],
   "id": "bdb01fc2dd141da7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T21:57:32.269161Z",
     "start_time": "2025-01-28T20:34:09.375699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)"
   ],
   "id": "d767b17cdae0cff2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-28 20:34:09,381] A new study created in memory with name: no-name-fd4bdd79-a73d-4b73-aba4-131330fcc7e3\n",
      "[I 2025-01-28 20:40:10,728] Trial 0 finished with value: 0.645 and parameters: {'hidden_dim': 512, 'num_layers': 2, 'dropout': 0.27456906318386587, 'lr': 0.0001542605632479653}. Best is trial 0 with value: 0.645.\n",
      "C:\\Users\\KomPhone\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4995289347837444 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-28 20:43:29,512] Trial 1 finished with value: 0.6553333333333333 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout': 0.4995289347837444, 'lr': 0.001801517854059461}. Best is trial 1 with value: 0.6553333333333333.\n",
      "C:\\Users\\KomPhone\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3820320663755761 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-28 20:46:46,223] Trial 2 finished with value: 0.6492222222222223 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout': 0.3820320663755761, 'lr': 0.0009542166913596821}. Best is trial 1 with value: 0.6553333333333333.\n",
      "[I 2025-01-28 20:50:29,815] Trial 3 finished with value: 0.653 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.3937889137309931, 'lr': 0.00039699537864364376}. Best is trial 1 with value: 0.6553333333333333.\n",
      "[I 2025-01-28 20:55:29,258] Trial 4 finished with value: 0.6521111111111111 and parameters: {'hidden_dim': 256, 'num_layers': 3, 'dropout': 0.4606910202676121, 'lr': 0.000693181030555464}. Best is trial 1 with value: 0.6553333333333333.\n",
      "[I 2025-01-28 20:59:20,377] Trial 5 finished with value: 0.6473333333333333 and parameters: {'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.17289710200129832, 'lr': 0.0030358418309509175}. Best is trial 1 with value: 0.6553333333333333.\n",
      "[I 2025-01-28 21:08:03,208] Trial 6 finished with value: 0.65 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout': 0.23045055612367454, 'lr': 0.008031267264551759}. Best is trial 1 with value: 0.6553333333333333.\n",
      "C:\\Users\\KomPhone\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.21756190233853534 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-28 21:11:20,252] Trial 7 finished with value: 0.6553333333333333 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout': 0.21756190233853534, 'lr': 0.0005328418017560168}. Best is trial 1 with value: 0.6553333333333333.\n",
      "C:\\Users\\KomPhone\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4547879447975328 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-28 21:14:36,382] Trial 8 finished with value: 0.6524444444444445 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout': 0.4547879447975328, 'lr': 0.0006457290067962315}. Best is trial 1 with value: 0.6553333333333333.\n",
      "C:\\Users\\KomPhone\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.24678548519240937 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-28 21:17:10,586] Trial 9 finished with value: 0.6497777777777778 and parameters: {'hidden_dim': 128, 'num_layers': 1, 'dropout': 0.24678548519240937, 'lr': 0.0022315013489860903}. Best is trial 1 with value: 0.6553333333333333.\n",
      "C:\\Users\\KomPhone\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.34930829696306576 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-28 21:19:45,591] Trial 10 finished with value: 0.6467777777777778 and parameters: {'hidden_dim': 256, 'num_layers': 1, 'dropout': 0.34930829696306576, 'lr': 0.009501716828491819}. Best is trial 1 with value: 0.6553333333333333.\n",
      "C:\\Users\\KomPhone\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.10328677745731396 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-28 21:23:02,989] Trial 11 finished with value: 0.6531111111111111 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout': 0.10328677745731396, 'lr': 0.00023256762651990322}. Best is trial 1 with value: 0.6553333333333333.\n",
      "C:\\Users\\KomPhone\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.17846413596218202 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-28 21:26:19,568] Trial 12 finished with value: 0.6505555555555556 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout': 0.17846413596218202, 'lr': 0.0020949869384489198}. Best is trial 1 with value: 0.6553333333333333.\n",
      "[I 2025-01-28 21:32:11,194] Trial 13 finished with value: 0.6516666666666666 and parameters: {'hidden_dim': 512, 'num_layers': 2, 'dropout': 0.320726349099694, 'lr': 0.0014745823058072646}. Best is trial 1 with value: 0.6553333333333333.\n",
      "C:\\Users\\KomPhone\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2033401879840663 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-28 21:35:29,026] Trial 14 finished with value: 0.6552222222222223 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout': 0.2033401879840663, 'lr': 0.004158802567435407}. Best is trial 1 with value: 0.6553333333333333.\n",
      "[I 2025-01-28 21:39:12,440] Trial 15 finished with value: 0.6513333333333333 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.1134833461505798, 'lr': 0.0003586634898083777}. Best is trial 1 with value: 0.6553333333333333.\n",
      "C:\\Users\\KomPhone\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.48373251675251155 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-28 21:42:30,126] Trial 16 finished with value: 0.6487777777777778 and parameters: {'hidden_dim': 512, 'num_layers': 1, 'dropout': 0.48373251675251155, 'lr': 0.0011551023184273644}. Best is trial 1 with value: 0.6553333333333333.\n",
      "[I 2025-01-28 21:51:13,697] Trial 17 finished with value: 0.6492222222222223 and parameters: {'hidden_dim': 512, 'num_layers': 3, 'dropout': 0.41092809113704937, 'lr': 0.004721619256266105}. Best is trial 1 with value: 0.6553333333333333.\n",
      "C:\\Users\\KomPhone\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3071891307496687 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-28 21:53:48,127] Trial 18 finished with value: 0.6543333333333333 and parameters: {'hidden_dim': 256, 'num_layers': 1, 'dropout': 0.3071891307496687, 'lr': 0.0005218936844827608}. Best is trial 1 with value: 0.6553333333333333.\n",
      "[I 2025-01-28 21:57:32,246] Trial 19 finished with value: 0.6418888888888888 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.26557930588122397, 'lr': 0.00012008308823524806}. Best is trial 1 with value: 0.6553333333333333.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 9. Final Model Training",
   "id": "cdcce85834abcdcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T22:09:24.440445Z",
     "start_time": "2025-01-28T22:09:24.404136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_params = study.best_params\n",
    "final_model = SentimentLSTM(\n",
    "    vocab_size=10001,\n",
    "    embed_dim=100,\n",
    "    output_dim=len(le.classes_),\n",
    "    pretrained_embeddings=embedding_matrix,\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)"
   ],
   "id": "a3a8135865bea67f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KomPhone\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4995289347837444 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T22:11:08.976418Z",
     "start_time": "2025-01-28T22:09:27.310699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train final model\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = train_epoch(final_model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(final_model, val_loader, criterion, device)\n",
    "    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')"
   ],
   "id": "7c3d4bd64bc3bf88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.8719 Acc: 0.5816 | Val Loss: 0.7895 Acc: 0.6272\n",
      "Epoch 2: Train Loss: 0.7542 Acc: 0.6523 | Val Loss: 0.7538 Acc: 0.6509\n",
      "Epoch 3: Train Loss: 0.6935 Acc: 0.6895 | Val Loss: 0.7494 Acc: 0.6504\n",
      "Epoch 4: Train Loss: 0.6338 Acc: 0.7212 | Val Loss: 0.7757 Acc: 0.6537\n",
      "Epoch 5: Train Loss: 0.5708 Acc: 0.7551 | Val Loss: 0.8242 Acc: 0.6491\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 10. Final Evaluation",
   "id": "d99f89368fe4cbbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T22:11:20.861886Z",
     "start_time": "2025-01-28T22:11:15.520422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss, test_acc = evaluate(final_model, test_loader, criterion, device)\n",
    "print(f'\\nFinal Test Performance: Loss: {test_loss:.4f} | Accuracy: {test_acc:.4f}')"
   ],
   "id": "b936adf37b47ac11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Performance: Loss: 0.8382 | Accuracy: 0.6419\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 11. Save the best parameters",
   "id": "ed2bfb317f0c2b67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T22:25:47.286405Z",
     "start_time": "2025-01-28T22:25:47.272377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open('params/best_bi_lstm_params.json', 'w') as f:\n",
    "    json.dump(best_params, f)"
   ],
   "id": "692d41a8a499165f",
   "outputs": [],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
